qwen batch推理效果差，因为加入了pad_token
[[BUG] 通过tokenizer和generate进行batch批量推理遇到问题，回答感觉了。不知道哪里写错了。 · Issue #159 · QwenLM/Qwen (github.com)](https://github.com/QwenLM/Qwen/issues/159)
```
Qwen的batch推理结果非常差是因为长度补齐导致的。即填补的token会影响generate的输出。  
我自己想出了一个方案：不要用官方的pad_id来填补，直接用0填补就可以。但是填补的时候需要注意，不能在最左侧或者最右侧填补。因为模型时decoder only,在左侧填补会引入大量空无意义的字符，在最右侧填补会导致原本句子的结束符跑到中间去了，模型在推理的时候找不到结束符。所以最好的方式就是在原始句子的最后，结束符之前填补无意义的字符，是比较好的选择。通过该种方式，就能保证输出结果的准确性。
```

生成被截断 要设置max_new_tokens,与max_length的区别是不包含input
贪婪解码，do_sample=false
设置no_repeat_ngram_size不知道为什么会影响模型性能


deepspeed1，2，3越来越慢