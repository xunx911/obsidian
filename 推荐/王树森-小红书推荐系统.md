# 举例的数字都是随便说的&论文
- Tiansheng Yao et al. Self-supervised Learning for Large-scale Item Recommendations. In CIKM, 2021

# 基础知识

## 转换流程

![[Pasted image 20240220114949.png]]
抖音没有曝光和点击,因为只能下滑看到一条视频

## 消费指标

![[Pasted image 20240220115204.png]]
不能把点击率作为唯一的优化指标,不然骗点击的标题会泛滥
追求短期消费指标是有意义的,但并不是衡量推荐系统好坏的根本指标(竭泽而渔,用户可能很快失去兴趣,而提高多样性虽然不会提升点击率,但可以提升用户粘性)

## 北极星指标

![[Pasted image 20240220115858.png]]
最关键的指标
用户规模(简单易懂,强相关)
消费与点击率冲突时,例如把多样性做好,用户点击率下降了但是使用了更长的时间,完全ok
发布(冷启动)

## 实验流程

![[Pasted image 20240220120802.png]]
小流量AB测试:对照组和实验组,判断新策略是否显著优于旧策略,如果是则加大流量,最终推全

# 推荐系统的链路

![[Pasted image 20240220121658.png]]
- 召回:用户刷新小红书时,系统调用几十条召回通道,每条通道召回几十到几百篇笔记,一共取回几千篇笔记
- 粗排:用规模小的机器学习模型排序和截断
- 精排:大规模神经网络模型打分,排序截断(小红书不截断)
- 重排:根据精排分数和多样性分数随机抽样得到几十篇笔记,然后把相似内容打散,并且插入广告和运营内容

## 召回通道

![[Pasted image 20240220122347.png]]
召回后还要过滤(例如不喜欢的笔记不喜欢的作者等)

## 重排

![[Pasted image 20240220122651.png]]
主要是考虑多样性,还要把相似的内容打散
![[Pasted image 20240220123018.png]]
抽样的依据:精排分数,多样性

## 粗排 精排

![[Pasted image 20240220122938.png]]

## 总结

![[Pasted image 20240220123114.png]]粗排时也会有一些规则,保证进入精排的笔记具有多样性
重排的规则非常复杂

# A/B测试

离线实验结果有收益,线上未必
![[Pasted image 20240220123422.png]]

## 随机分桶

![[Pasted image 20240220123609.png]]![[Pasted image 20240220123618.png]]每个桶不同的策略/参数

## 分层实验

目标:解决流量不够用的问题
![[Pasted image 20240220123844.png]]
![[Pasted image 20240220123905.png]]
一般来说不同层的实验不会对另一层造成影响,所以是正交
举例
![[Pasted image 20240220124024.png]]![[Pasted image 20240220124216.png]]不同的召回实验不能同时作用于一个用户身上
![[Pasted image 20240220124255.png]]![[Pasted image 20240220142607.png]]

## Holdout机制

![[Pasted image 20240220142950.png]]

## 实验推全 & 反转实验

![[Pasted image 20240220151612.png]]![[Pasted image 20240220151634.png]]
![[Pasted image 20240220151622.png]]![[Pasted image 20240220151710.png]]

## 总结

![[Pasted image 20240220151753.png]]

# 召回

## ItemCF

![[Pasted image 20240220163747.png]]
![[Pasted image 20240220152047.png]]从用户的行为中挖掘物品的相似性
![[Pasted image 20240220152158.png]]![[Pasted image 20240220152204.png]]![[Pasted image 20240220152256.png]]

### 完整流程

![[Pasted image 20240220160903.png]]![[Pasted image 20240220161040.png]]
⽤索引，离线计算量⼤，线上计算量⼩

## Swing召回通道(ItemCF变体)

### ItemCF的不足

![[Pasted image 20240220163349.png]]假设两篇不相关的笔记恰巧分享到了一个vx群,导致一些用户同时交互过这两篇笔记,系统就会误判

### Swing模型

![[Pasted image 20240220163909.png]]![[Pasted image 20240220163917.png]]

### 总结

![[Pasted image 20240220163933.png]]

## UserCF

![[Pasted image 20240220164133.png]]![[Pasted image 20240221145714.png]]![[Pasted image 20240221145722.png]]
越热门的物品应该影响越小
![[Pasted image 20240221150012.png]]![[Pasted image 20240221150021.png]]

### 完整流程

![[Pasted image 20240221150151.png]]
![[Pasted image 20240221150234.png]]

## 矩阵补充

![[Pasted image 20240221160442.png]]![[Pasted image 20240221160458.png]]![[Pasted image 20240221160519.png]]![[Pasted image 20240221160536.png]]![[Pasted image 20240221160615.png]]![[Pasted image 20240221160712.png]]![[Pasted image 20240221160725.png]]![[Pasted image 20240221160824.png]]![[Pasted image 20240221160922.png]]
![[Pasted image 20240221161105.png]]线上实时计算不现实->如何加速最近邻查找避免暴力枚举

### 近似最近邻查找

几亿个物品只做几万次内积
![[Pasted image 20240221161250.png]]![[Pasted image 20240221163202.png]]给物品向量划分区域,每个区域用一个单位向量表示,建立索引,作为key,能取回区域中的所有点
![[Pasted image 20240221163424.png]]
先找最近的区域,再计算区域内的点即可

### 总结

![[Pasted image 20240221163510.png]]![[Pasted image 20240221163524.png]]

## 双塔模型

![[Pasted image 20240221163651.png]]用户离散特征:例如喜欢的话题,所在的城市,性别等,每个特征单独用一个embedding层
用户连续特征:例如金额等
![[Pasted image 20240221163821.png]]
与矩阵补充相比,双塔模型使用了更多特征
![[Pasted image 20240221163928.png]]

### 训练

![[Pasted image 20240221164031.png]]

#### 正负样本的选择

![[Pasted image 20240221164112.png]]
负样本的选择比较讲究,看论文

#### pointwise

![[Pasted image 20240221164216.png]]

#### pairwise

![[Pasted image 20240221164349.png]]
![[Pasted image 20240221164413.png]]![[Pasted image 20240221164449.png]]
m是超参数,需要调
![[Pasted image 20240221171044.png]]

#### listwise

![[Pasted image 20240221171740.png]]![[Pasted image 20240223103247.png]]

### 总结

![[Pasted image 20240223103449.png]]
![[Pasted image 20240223112538.png]]
前期融合,通常是排序,双塔是后期融合

### 线上召回和更新

![[Pasted image 20240223123806.png]]![[Pasted image 20240223123816.png]]注意用户的特征向量是线上算的,而不用离线存储
![[Pasted image 20240223123917.png]]![[Pasted image 20240223123924.png]]![[Pasted image 20240223124120.png]]

#### 更新

![[Pasted image 20240223124140.png]]
增量更新可以几小时的更新,例如早上刷小红书,有了新的兴趣
![[Pasted image 20240223124358.png]]增量更新只更新用户塔的embedding层,全量更新才会更新全连接层.
![[Pasted image 20240223125027.png]]![[Pasted image 20240223125034.png]]全量更新用的是全量模型而不是增量模型

>[!question] 能否只做增量更新，不做全量更新?

no,效果不好,短时间的数据有偏差,与全天数据相比偏差巨大
![[Pasted image 20240223125603.png]]
### 正样本

选对正负样本,作用大于改进模型结构

![[Pasted image 20240223112914.png]]抛弃正样本的概率与样本的点击次数正相关

### 负样本

![[Pasted image 20240223122123.png]]

#### 简单负样本

![[Pasted image 20240223122159.png]]![[Pasted image 20240223122227.png]]![[Pasted image 20240223122417.png]]![[Pasted image 20240223122428.png]]
对于第一个用户来说,第二个物品相当于全体中随机抽样的,大概率不喜欢![[Pasted image 20240223122606.png]]打压负样本过大了
![[Pasted image 20240223122720.png]]训练时做调整来纠偏,线上召回时依然只用cos

#### 困难负样本

![[Pasted image 20240223122845.png]]困难指的是分类难
![[Pasted image 20240223123044.png]]
#### 常见错误
![[Pasted image 20240223123234.png]]
>[!caution] 曝光但没有点击的样本不能作为训练召回模型的负样本

![[Pasted image 20240223123318.png]]
召回是为了区分不感兴趣和比较感兴趣的
而排序是为了区分比较感兴趣和非常感兴趣的

#### 总结

![[Pasted image 20240223123545.png]]
### 总结
![[Pasted image 20240223125823.png]]![[Pasted image 20240223125936.png]]![[Pasted image 20240223125943.png]]
### 自监督学习
目的:优化物品塔
![[Pasted image 20240223131044.png]]![[Pasted image 20240223135342.png]]![[Pasted image 20240223135346.png]]![[Pasted image 20240223135448.png]]![[Pasted image 20240223135605.png]]![[Pasted image 20240223135919.png]]
自监督学习将物品转换为不同的表征,相同物品应该近,不同的应该远![[Pasted image 20240223140150.png]]![[Pasted image 20240223140159.png]]![[Pasted image 20240223140206.png]]![[Pasted image 20240223140304.png]]
原本可能是数码embedding和摄影embedding做计算,mask后直接丢掉变成缺失值的default
![[Pasted image 20240223140425.png]]
区别:mask是丢掉所有
![[Pasted image 20240223140457.png]]![[Pasted image 20240223141927.png]]
特征之间存在关联,所以要一起mask掉
![[Pasted image 20240223142040.png]]![[Pasted image 20240223142125.png]]![[Pasted image 20240223142159.png]]![[Pasted image 20240223142210.png]]![[Pasted image 20240223142355.png]]![[Pasted image 20240223142402.png]]![[Pasted image 20240223142743.png]]

#### 总结
![[Pasted image 20240223142806.png]]![[Pasted image 20240223142932.png]]
>[!note] 自监督效果非常好

## Deep Retrieval
### 区别
![[Pasted image 20240223143118.png]]
当作路径,不做相似最近邻查找
![[Pasted image 20240223143311.png]]
### 索引
![[Pasted image 20240223143431.png]]![[Pasted image 20240223143524.png]]
给定一个路径 取回多个物品作为召回的结果
### 预估模型
![[Pasted image 20240223143644.png]]![[Pasted image 20240223145423.png]]![[Pasted image 20240223145435.png]]
p1,p2,p3对应 L1,L2,L3
### 线上召回
![[Pasted image 20240223150110.png]]
### Beam Search
![[Pasted image 20240223150814.png]]![[Pasted image 20240223151217.png]]![[Pasted image 20240223151230.png]]![[Pasted image 20240223151237.png]]![[Pasted image 20240223152432.png]]![[Pasted image 20240223153606.png]]![[Pasted image 20240223153629.png]]![[Pasted image 20240223153638.png]]![[Pasted image 20240223153705.png]]
### 训练
![[Pasted image 20240223163749.png]]![[Pasted image 20240223163800.png]]![[Pasted image 20240223164025.png]]![[Pasted image 20240223164158.png]]![[Pasted image 20240223164207.png]]![[Pasted image 20240223170143.png]]![[Pasted image 20240223170552.png]]
固定J-1条路径,物品与pathl的相关性越高,loss越小
#### 小结
交替训练更新
![[Pasted image 20240223171630.png]]更新神经网络时物品作为中介
![[Pasted image 20240224125915.png]]用户作为中介,给定一个物品,找到点击过物品的所有用户,然后神经网络计算用户对路径的兴趣分数,把分数加起来就是物品与路径的相关性
#### 总结
召回分两阶段,用户->路径,路径->物品
![[Pasted image 20240224130156.png]]
![[Pasted image 20240224130232.png]]
>[!note]  双塔模型本质是用向量表征作为用户和物品之间的中介,而DR是path

![[Pasted image 20240224143917.png]]
## 其他召回路径
### 地理位置召回
![[Pasted image 20240224175012.png]]
经纬度编码成二进制哈希码
给定用户的geohash,召回附近区域的新的优质笔记,得是优质笔记
![[Pasted image 20240224175139.png]]
哪些笔记用户感兴趣留待排序模型判断
![[Pasted image 20240224175219.png]]
### 作者召回

![[Pasted image 20240224175410.png]]![[Pasted image 20240224175415.png]]![[Pasted image 20240224175549.png]]
相似作者的相似度计算,例如粉丝的重合度
### 缓存召回
![[Pasted image 20240224175629.png]]
![[Pasted image 20240224175753.png]]
比较复杂的规则,例如扶持低曝光的笔记,设计规则让低曝光的多保留
## 曝光过滤&Bloom Filter
![[Pasted image 20240224180306.png]]
![[Pasted image 20240224180342.png]]
