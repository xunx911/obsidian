# 举例的数字都是随便说的&论文

- 对比学习 Tiansheng Yao et al. Self-supervised Learning for Large-scale Item Recommendations. In CIKM, 2021
- MMoE(Google) Jiaqi Ma et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts. In KDD, 2018
- 解决极化问题(Youtube) Zhe Zhao et al. Recommending What Video to Watch Next: A Multitask Ranking System. In RecSys, 2019
- 时长建模(Youtube) Paul Covington, Jay Adams, & Emre Sargin. Deep Neural Networks for YouTube Recommendations. In RecSys, 2016
- 三塔模型(阿里)  Zhe Wang et al. COLD: Towards the Next Generation of Pre-Ranking System. In DLPKDD, 2020.
- 交叉网络 Ruoxi Wang et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.
- DIN(阿里) Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.
- SIM  Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020

# 基础知识

## 转换流程

![[Pasted image 20240220114949.png]]
抖音没有曝光和点击,因为只能下滑看到一条视频

## 消费指标

![[Pasted image 20240220115204.png]]
不能把点击率作为唯一的优化指标,不然骗点击的标题会泛滥
追求短期消费指标是有意义的,但并不是衡量推荐系统好坏的根本指标(竭泽而渔,用户可能很快失去兴趣,而提高多样性虽然不会提升点击率,但可以提升用户粘性)

## 北极星指标

![[Pasted image 20240220115858.png]]
最关键的指标
用户规模(简单易懂,强相关)
消费与点击率冲突时,例如把多样性做好,用户点击率下降了但是使用了更长的时间,完全ok
发布(冷启动)

## 实验流程

![[Pasted image 20240220120802.png]]
小流量AB测试:对照组和实验组,判断新策略是否显著优于旧策略,如果是则加大流量,最终推全

# 推荐系统的链路

![[Pasted image 20240220121658.png]]
- 召回:用户刷新小红书时,系统调用几十条召回通道,每条通道召回几十到几百篇笔记,一共取回几千篇笔记
- 粗排:用规模小的机器学习模型排序和截断
- 精排:大规模神经网络模型打分,排序截断(小红书不截断)
- 重排:根据精排分数和多样性分数随机抽样得到几十篇笔记,然后把相似内容打散,并且插入广告和运营内容

## 召回通道

![[Pasted image 20240220122347.png]]
召回后还要过滤(例如不喜欢的笔记不喜欢的作者等)

## 重排

![[Pasted image 20240220122651.png]]
主要是考虑多样性,还要把相似的内容打散
![[Pasted image 20240220123018.png]]
抽样的依据:精排分数,多样性

## 粗排 精排

![[Pasted image 20240220122938.png]]

## 总结

![[Pasted image 20240220123114.png]]粗排时也会有一些规则,保证进入精排的笔记具有多样性
重排的规则非常复杂

# A/B测试

离线实验结果有收益,线上未必
![[Pasted image 20240220123422.png]]

## 随机分桶

![[Pasted image 20240220123609.png]]![[Pasted image 20240220123618.png]]每个桶不同的策略/参数

## 分层实验

目标:解决流量不够用的问题
![[Pasted image 20240220123844.png]]
![[Pasted image 20240220123905.png]]
一般来说不同层的实验不会对另一层造成影响,所以是正交
举例
![[Pasted image 20240220124024.png]]![[Pasted image 20240220124216.png]]不同的召回实验不能同时作用于一个用户身上
![[Pasted image 20240220124255.png]]![[Pasted image 20240220142607.png]]

## Holdout机制

![[Pasted image 20240220142950.png]]

## 实验推全 & 反转实验

![[Pasted image 20240220151612.png]]![[Pasted image 20240220151634.png]]
![[Pasted image 20240220151622.png]]![[Pasted image 20240220151710.png]]

## 总结

![[Pasted image 20240220151753.png]]

# 召回

## ItemCF

![[Pasted image 20240220163747.png]]
![[Pasted image 20240220152047.png]]从用户的行为中挖掘物品的相似性
![[Pasted image 20240220152158.png]]![[Pasted image 20240220152204.png]]![[Pasted image 20240220152256.png]]

### 完整流程

![[Pasted image 20240220160903.png]]![[Pasted image 20240220161040.png]]
⽤索引，离线计算量⼤，线上计算量⼩

## Swing召回通道(ItemCF变体)

### ItemCF的不足

![[Pasted image 20240220163349.png]]假设两篇不相关的笔记恰巧分享到了一个vx群,导致一些用户同时交互过这两篇笔记,系统就会误判

### Swing模型

![[Pasted image 20240220163909.png]]![[Pasted image 20240220163917.png]]

### 总结

![[Pasted image 20240220163933.png]]

## UserCF

![[Pasted image 20240220164133.png]]![[Pasted image 20240221145714.png]]![[Pasted image 20240221145722.png]]
越热门的物品应该影响越小
![[Pasted image 20240221150012.png]]![[Pasted image 20240221150021.png]]

### 完整流程

![[Pasted image 20240221150151.png]]
![[Pasted image 20240221150234.png]]

## 矩阵补充

![[Pasted image 20240221160442.png]]![[Pasted image 20240221160458.png]]![[Pasted image 20240221160519.png]]![[Pasted image 20240221160536.png]]![[Pasted image 20240221160615.png]]![[Pasted image 20240221160712.png]]![[Pasted image 20240221160725.png]]![[Pasted image 20240221160824.png]]![[Pasted image 20240221160922.png]]
![[Pasted image 20240221161105.png]]线上实时计算不现实->如何加速最近邻查找避免暴力枚举

### 近似最近邻查找

几亿个物品只做几万次内积
![[Pasted image 20240221161250.png]]![[Pasted image 20240221163202.png]]给物品向量划分区域,每个区域用一个单位向量表示,建立索引,作为key,能取回区域中的所有点
![[Pasted image 20240221163424.png]]
先找最近的区域,再计算区域内的点即可

### 总结

![[Pasted image 20240221163510.png]]![[Pasted image 20240221163524.png]]

## 双塔模型

![[Pasted image 20240221163651.png]]用户离散特征:例如喜欢的话题,所在的城市,性别等,每个特征单独用一个embedding层
用户连续特征:例如金额等
![[Pasted image 20240221163821.png]]
与矩阵补充相比,双塔模型使用了更多特征
![[Pasted image 20240221163928.png]]

### 训练

![[Pasted image 20240221164031.png]]

#### 正负样本的选择

![[Pasted image 20240221164112.png]]
负样本的选择比较讲究,看论文

#### pointwise

![[Pasted image 20240221164216.png]]

#### pairwise

![[Pasted image 20240221164349.png]]
![[Pasted image 20240221164413.png]]![[Pasted image 20240221164449.png]]
m是超参数,需要调
![[Pasted image 20240221171044.png]]

#### listwise

![[Pasted image 20240221171740.png]]![[Pasted image 20240223103247.png]]

### 总结

![[Pasted image 20240223103449.png]]
![[Pasted image 20240223112538.png]]
前期融合,通常是排序,双塔是后期融合

### 线上召回和更新

![[Pasted image 20240223123806.png]]![[Pasted image 20240223123816.png]]注意用户的特征向量是线上算的,而不用离线存储
![[Pasted image 20240223123917.png]]![[Pasted image 20240223123924.png]]![[Pasted image 20240223124120.png]]

#### 更新

![[Pasted image 20240223124140.png]]
增量更新可以几小时的更新,例如早上刷小红书,有了新的兴趣
![[Pasted image 20240223124358.png]]增量更新只更新用户塔的embedding层,全量更新才会更新全连接层.
![[Pasted image 20240223125027.png]]![[Pasted image 20240223125034.png]]全量更新用的是全量模型而不是增量模型

>[!question] 能否只做增量更新，不做全量更新?

no,效果不好,短时间的数据有偏差,与全天数据相比偏差巨大
![[Pasted image 20240223125603.png]]

### 正样本

选对正负样本,作用大于改进模型结构

![[Pasted image 20240223112914.png]]抛弃正样本的概率与样本的点击次数正相关

### 负样本

![[Pasted image 20240223122123.png]]

#### 简单负样本

![[Pasted image 20240223122159.png]]![[Pasted image 20240223122227.png]]![[Pasted image 20240223122417.png]]![[Pasted image 20240223122428.png]]
对于第一个用户来说,第二个物品相当于全体中随机抽样的,大概率不喜欢![[Pasted image 20240223122606.png]]打压负样本过大了
![[Pasted image 20240223122720.png]]训练时做调整来纠偏,线上召回时依然只用cos

#### 困难负样本

![[Pasted image 20240223122845.png]]困难指的是分类难
![[Pasted image 20240223123044.png]]

#### 常见错误

![[Pasted image 20240223123234.png]]
>[!caution] 曝光但没有点击的样本不能作为训练召回模型的负样本

![[Pasted image 20240223123318.png]]
召回是为了区分不感兴趣和比较感兴趣的
而排序是为了区分比较感兴趣和非常感兴趣的

#### 总结

![[Pasted image 20240223123545.png]]

### 总结

![[Pasted image 20240223125823.png]]![[Pasted image 20240223125936.png]]![[Pasted image 20240223125943.png]]

### 自监督学习

目的:优化物品塔
![[Pasted image 20240223131044.png]]![[Pasted image 20240223135342.png]]![[Pasted image 20240223135346.png]]![[Pasted image 20240223135448.png]]![[Pasted image 20240223135605.png]]![[Pasted image 20240223135919.png]]
自监督学习将物品转换为不同的表征,相同物品应该近,不同的应该远![[Pasted image 20240223140150.png]]![[Pasted image 20240223140159.png]]![[Pasted image 20240223140206.png]]![[Pasted image 20240223140304.png]]
原本可能是数码embedding和摄影embedding做计算,mask后直接丢掉变成缺失值的default
![[Pasted image 20240223140425.png]]
区别:mask是丢掉所有
![[Pasted image 20240223140457.png]]![[Pasted image 20240223141927.png]]
特征之间存在关联,所以要一起mask掉
![[Pasted image 20240223142040.png]]![[Pasted image 20240223142125.png]]![[Pasted image 20240223142159.png]]![[Pasted image 20240223142210.png]]![[Pasted image 20240223142355.png]]![[Pasted image 20240223142402.png]]![[Pasted image 20240223142743.png]]

#### 总结

![[Pasted image 20240223142806.png]]![[Pasted image 20240223142932.png]]
>[!note] 自监督效果非常好

## Deep Retrieval

### 区别

![[Pasted image 20240223143118.png]]
当作路径,不做相似最近邻查找
![[Pasted image 20240223143311.png]]

### 索引

![[Pasted image 20240223143431.png]]![[Pasted image 20240223143524.png]]
给定一个路径 取回多个物品作为召回的结果

### 预估模型

![[Pasted image 20240223143644.png]]![[Pasted image 20240223145423.png]]![[Pasted image 20240223145435.png]]
p1,p2,p3对应 L1,L2,L3

### 线上召回

![[Pasted image 20240223150110.png]]

### Beam Search

![[Pasted image 20240223150814.png]]![[Pasted image 20240223151217.png]]![[Pasted image 20240223151230.png]]![[Pasted image 20240223151237.png]]![[Pasted image 20240223152432.png]]![[Pasted image 20240223153606.png]]![[Pasted image 20240223153629.png]]![[Pasted image 20240223153638.png]]![[Pasted image 20240223153705.png]]

### 训练

![[Pasted image 20240223163749.png]]![[Pasted image 20240223163800.png]]![[Pasted image 20240223164025.png]]![[Pasted image 20240223164158.png]]![[Pasted image 20240223164207.png]]![[Pasted image 20240223170143.png]]![[Pasted image 20240223170552.png]]
固定J-1条路径,物品与pathl的相关性越高,loss越小

#### 小结

交替训练更新
![[Pasted image 20240223171630.png]]更新神经网络时物品作为中介
![[Pasted image 20240224125915.png]]用户作为中介,给定一个物品,找到点击过物品的所有用户,然后神经网络计算用户对路径的兴趣分数,把分数加起来就是物品与路径的相关性

#### 总结

召回分两阶段,用户->路径,路径->物品
![[Pasted image 20240224130156.png]]
![[Pasted image 20240224130232.png]]
>[!note]  双塔模型本质是用向量表征作为用户和物品之间的中介,而DR是path

![[Pasted image 20240224143917.png]]

## 其他召回路径

### 地理位置召回

![[Pasted image 20240224175012.png]]
经纬度编码成二进制哈希码
给定用户的geohash,召回附近区域的新的优质笔记,得是优质笔记
![[Pasted image 20240224175139.png]]
哪些笔记用户感兴趣留待排序模型判断
![[Pasted image 20240224175219.png]]

### 作者召回

![[Pasted image 20240224175410.png]]![[Pasted image 20240224175415.png]]![[Pasted image 20240224175549.png]]
相似作者的相似度计算,例如粉丝的重合度

### 缓存召回

![[Pasted image 20240224175629.png]]
![[Pasted image 20240224175753.png]]
比较复杂的规则,例如扶持低曝光的笔记,设计规则让低曝光的多保留

## 曝光过滤&Bloom Filter

![[Pasted image 20240224180306.png]]
![[Pasted image 20240224180342.png]]
![[Pasted image 20240226145540.png]]
是二进制向量,要么0要么1
>[!question] 如何用k个哈希函数做曝光过滤

![[Pasted image 20240226145814.png]]
初始是m维的向量,把已曝光的物品id哈希到的位置置为1
![[Pasted image 20240226150231.png]]
召回的物品映射的三种情况
![[Pasted image 20240226150331.png]]
k=3时
只要有一个位置不为1 就未曝光
![[Pasted image 20240226150447.png]]

### 误伤概率

![[Pasted image 20240226150539.png]]![[Pasted image 20240226150921.png]]

### 曝光过滤的链路

![[Pasted image 20240226151216.png]]
实时流处理的要求很高

### 缺点

![[Pasted image 20240226151247.png]]想要删除一个物品,需要重新计算整个集合的二进制向量

# 排序

## 多目标排序模型

先不区分粗排和精排
![[Pasted image 20240226153430.png]]
![[Pasted image 20240226153458.png]]
![[Pasted image 20240226153542.png]]权重是做A/B测试调出来的
加权和也有改进
![[Pasted image 20240226153736.png]]
用户特征:用户id  用户画像
物品特征:物品id  物品画像  作者信息
统计特征:用户统计特征(比如用户在过去30天中曝光了多少篇笔记  点击了多少篇笔记)  物品统计特征(被曝光了多少次,点击点赞了多少次)
场景特征:当前时间(周末,工作日)  用户所在地点(同城)

### 训练

![[Pasted image 20240226154102.png]]模型做出的预估拟合真实目标
y是0或1,例如1 0 0 1代表有点击 无点赞 无点赞 有转发
二元分类 用交叉熵
![[Pasted image 20240226154247.png]]

### 预估值校准

>[!question] 为什么要做校准

![[Pasted image 20240226154441.png]]
![[Pasted image 20240226161618.png]]

### Multi-gate Mixture-of-Experts(MMoE)

![[Pasted image 20240226162854.png]]
三个神经网络不共享参数,实践中通常4个或者8个
![[Pasted image 20240226162955.png]]特征向量输入另一个神经网络,输出对应三个专家的权重
![[Pasted image 20240226163046.png]]
q123也是后续加权的权重
![[Pasted image 20240226163135.png]]
这些神经网络都是1个或多个全连接层(示例)
假如有10个目标,就有10个权重
专家数量是超参数,需要调

#### 极化现象(Polarization)

实践中会出现下列现象
![[Pasted image 20240226163356.png]]
![[Pasted image 20240226163443.png]]
![[Pasted image 20240226163507.png]]强迫部分专家做预测,因为如果发生极化现象,那么那个专家被mask时结果会非常差,所以可以避免极化
MMoE不是用了就有效果的,可能是实现不够好或者业务不够匹配,效果不好的原因不清楚

### 融合预估分数

![[Pasted image 20240226164820.png]]
简单又常用
![[Pasted image 20240226170322.png]]
$p_{time}$的意思是预估用户观看时长,$\alpha_1,\alpha_2$都是超参数,A/B测试调
有很多预估指标,连乘
![[Pasted image 20240226170508.png]]
以$p_{time}$为例,时长最长,排名越小,得分越高,与之前不同,这里用的是排名
![[Pasted image 20240226170731.png]]
$\alpha$是超参数,如果都是1,则是营收,有很明确的物理意义

### 视频播放建模

#### 视频播放时长

![[Pasted image 20240226171012.png]]
![[Pasted image 20240226171104.png]]

#### Youtu的时长建模

![[Pasted image 20240226171207.png]]
这个神经网络被所有任务共享
以播放时长为例
t的意思是用户实际观看视频的时长
t越大,y越大
用p拟合y,最小化交叉熵,如果p=y,则exp(z)=t所以可以用exp(z)作为播放时长的预估
![[Pasted image 20240226183603.png]]
 所以训练时用p 训练完成后就没用了, 线上计算时用exp(z)即可
 ![[Pasted image 20240226184437.png]]
 ![[Pasted image 20240226184506.png]]
 #### 视频完播

![[Pasted image 20240226184659.png]]![[Pasted image 20240226184723.png]]
![[Pasted image 20240226184821.png]]
实践中不能将完播率直接用在融分公式中,否则对短视频有利而对长视频不利
![[Pasted image 20240226185014.png]]

## 排序模型的特征

### 用户画像

![[Pasted image 20240226185145.png]]
用户id的embedding通常32维  64维
人口统计学信息  账号信息相差都很大
感兴趣的类目可能是用户提取的  也有可能是算法提取的
用户id和物品id虽然本身不携带额外信息,但是embedding的价值都非常高

### 物品画像

![[Pasted image 20240226185712.png]]
GeoHash是经纬度的编码,表示一个长方形的区域
内容信息量  图片美学是算法打分  事先训练nlp和cv模型

### 用户统计特征

![[Pasted image 20240226185834.png]]

### 笔记统计特征

![[Pasted image 20240226185915.png]]
例如笔记30天指标很高,但是最近1天的指标很差,说明过时了,不应该给更多流量

### 场景特征

![[Pasted image 20240226190111.png]]
随用户请求传来的
安卓和苹果用户的点赞差异也非常显著

### 特征处理

![[Pasted image 20240226190237.png]]
离散特征处理起来比较容易  存储占用较小
连续特征平滑解决异常值的问题,例如一些曝光特别多的笔记,数百万 直接计算会导致梯度计算等很奇怪
平滑点赞率去掉偶然性造成的波动

### 小结

![[Pasted image 20240226190549.png]]
各个特征的各个特征都很有用

### 特征覆盖率

![[Pasted image 20240226190616.png]]

### 数据服务

3个数据源,存储在内存服务器中
![[Pasted image 20240226190720.png]]![[Pasted image 20240226190800.png]]
![[Pasted image 20240226190816.png]]用户画像存储库压力小  物品画像数据库压力非常大
所以用户存储库可以特征多 大一些  而物品最好不要放很大的向量
这两个静态有时可以缓存在排序服务器本地加速计算
但是统计数据不行,要尽快刷新数据库
![[Pasted image 20240226191607.png]]
tensorflow返回分数,排序服务器用融合分数 多样性分数 业务规则给笔记排序,最后把分最高的返回
实际上的系统会复杂很多

## 粗排

### 粗排vs精排

![[Pasted image 20240226191743.png]]

### 回顾精排&双塔模型

![[Pasted image 20240226191846.png]]
shared bottom
![[Pasted image 20240226191908.png]]
![[Pasted image 20240226191934.png]]
![[Pasted image 20240226191958.png]]

### 粗排的三塔模型

![[Pasted image 20240226192146.png]]交叉特征是指用户特征和场景特征交叉
介于前期融合和后期融合之间(前期融合是把最下面的特征concatenation,而这里是把输出的三个特征concatenation)
![[Pasted image 20240226192400.png]]物品特征相对稳定,可以缓存物品塔向量,每隔一段时间刷新一次
每当一个一个用户点击  一个物品曝光,统计特征就发生了变化,所以交叉塔必须小,通常来说交叉塔只有一层,宽度也比较小
![[Pasted image 20240226192730.png]]
代价>交叉塔的n次推理
![[Pasted image 20240226192804.png]]
粗排模型的理念是尽量减少线上的计算量

# 特征交叉

## Factorized Machine(FM)

几年前比较流行,现在已经不常用了

### 线性模型

![[Pasted image 20240226193057.png]]
特征之间没有交叉

### 二阶交叉特征

![[Pasted image 20240226193233.png]]
例如房子的面积和房价,就需要交叉
如果d很大 计算代价就太大了,而且容易过拟合
![[Pasted image 20240226193421.png]]
U是对称矩阵,用V来近似
![[Pasted image 20240226193515.png]]
如此 就减少了参数量
![[Pasted image 20240226194519.png]]
![[Pasted image 20240226194539.png]]在推荐系统中,FM显著比线性模型效果好,2010时逻辑回归加FM效果很好,现在过时了

## 深度交叉网络(DCN)

用来代替简单的全连接网络

### 之前的模型替换

![[Pasted image 20240226195924.png]]![[Pasted image 20240226195945.png]]![[Pasted image 20240226195957.png]]

### 交叉层(Cross Layer)

![[Pasted image 20240226200107.png]]
第i个交叉层
把xi输入全连接层得到y形状相同,与x0做哈达玛乘积(逐元素相乘),等到z与x0形状一样,与xi相加,类似与resnet中的跳跃连接skip connection
![[Pasted image 20240226200342.png]]
xi+1是输出,x0和xi是输入
参数全在全连接层
![[Pasted image 20240226200432.png]]

### 交叉网络(Cross Network)

![[Pasted image 20240226200627.png]]![[Pasted image 20240226200642.png]]
x0需要输入到后面的交叉层
![[Pasted image 20240226200745.png]]

### 深度交叉网络(Deep & Cross Network)

![[Pasted image 20240226200907.png]]
concatenation后输入到两个网络中,各输出一个向量,concatenation后输入到全连接层,输出一个向量
![[Pasted image 20240226201028.png]]
比全连接网络效果更好,召回和排序中都可以使用

## LHUC网络结构

有效但只能用于精排
![[Pasted image 20240227153940.png]]![[Pasted image 20240227153954.png]]

### 语音识别中的LHUC

![[Pasted image 20240227154111.png]]神经网络包含多个全连接层,最后得到的每个向量都介于0到2
![[Pasted image 20240227154153.png]]
哈达玛乘积之后输入全连接层
神经网络跟上面一样,
![[Pasted image 20240227154248.png]]
语音信号中的特征结合说话者的特征,做到了个性化
示例中是2层   实践中可以更深
![[Pasted image 20240227154331.png]]
这样的介于0到2的向量与信号特征做乘积就可以放大一些特征,缩小一些特征,做到个性化

### 推荐系统排序模型中的LHUC

![[Pasted image 20240227154433.png]]

## SENet & Bilinear Cross

对排序有收益
![[Pasted image 20240227154610.png]]![[Pasted image 20240227154624.png]]
![[Pasted image 20240227154703.png]]
设m维特征,每个特征是一个k维向量,所有特征是m\*k的矩阵
做完avgpool之后的向量,每个元素对应一个离散特征,比如第一个元素对应用户id
r为压缩比例>1
然后再恢复
![[Pasted image 20240227154851.png]]
用m\*1维的向量做加权
![[Pasted image 20240227154935.png]]
例如学出物品id embedding对任务重要性不高,就给它降权
![[Pasted image 20240227155052.png]]维度可以不同,毕竟只是加权
![[Pasted image 20240227155149.png]]
也就是说,SENet会根据所有特征,判断特征的重要性

### Field间特征交叉

![[Pasted image 20240227155310.png]]xi和xj是两个特征,   哈达玛乘积的代价有些大,不过都要求形状一样
![[Pasted image 20240227155800.png]]![[Pasted image 20240227155948.png]]形状可以不同,参数矩阵太大,需要人工选pair

![[Pasted image 20240227160027.png]]
人工指定特征交叉

![[Pasted image 20240227160121.png]]

### FIBiNet

![[Pasted image 20240227160200.png]]![[Pasted image 20240227160713.png]]
作者认为最下层的Bilinear cross是多余
![[Pasted image 20240227160746.png]]
与连续特征concatenate输入

# 用户行为序列

## 用户行为序列建模
last n意思用户最后交互过的n个物品
很有效,用于召回和排序模型  指标都会大涨
![[Pasted image 20240227160949.png]]![[Pasted image 20240227160957.png]]
最后平均作为用户的一种特征
### LastN特征
![[Pasted image 20240227161022.png]]
#### 小红书的实践
 ![[Pasted image 20240227161121.png]]
 attention的效果比取平均效果好,但是计算量大
 最后把向量拼起来
 不止物品id  也有物品类目等等
 每家公司用的方法都不太一样,主要原因是系统基建水平不一样
 ## DIN模型
 ![[Pasted image 20240227161343.png]]
 ![[Pasted image 20240227161444.png]]
 红色是用户交互过的lastN物品的向量表征,
 蓝色是  比如粗排选出物品就是精排的候选物品
 ![[Pasted image 20240227161602.png]]
 分别计算相似度
 ![[Pasted image 20240227161616.png]]
 把$\alpha与x$相乘,得到lastN向量的加权和,权重就是相似度
 ![[Pasted image 20240227161723.png]]![[Pasted image 20240227162329.png]]![[Pasted image 20240227162350.png]]![[Pasted image 20240227162523.png]]
 ## SIM模型
 主要目的是保留用户的长期兴趣
 ### DIN的缺点
 ![[Pasted image 20240227162642.png]]
 做过实验,提升用户行为序列的长度,能提升所有指标,但是不划算,增加的计算量太大,收益不够高
 ![[Pasted image 20240227162818.png]]
 因为无关的物品权重接近0,所以排除掉也可以,这样就可以降低计算量
 ![[Pasted image 20240227162924.png]]
 如果不用SIM,n可能就是一两百
 topk  例如用类目信息把1000个变成了100个
 ### 查找
 ![[Pasted image 20240227163102.png]]
 除非基建很强,hard search就可以了
 ### 注意力机制
 ![[Pasted image 20240227163214.png]]
 紫色向量几乎不变,被排除的物品相似度本就接近0
 ![[Pasted image 20240227163251.png]]
 小trick   把两个向量concatenation
 ![[Pasted image 20240227163345.png]]
 候选物品不需要时间的embedding
 ![[Pasted image 20240227163455.png]]
 使用时间信息能得到显著提升
 ![[Pasted image 20240227163520.png]]
 # 多样性
 ## 推荐系统中的多样性
 ### 物品相似度的度量
 ![[Pasted image 20240227173843.png]]
 ![[Pasted image 20240227173855.png]]
 ![[Pasted image 20240227173920.png]]
 头部现象严重,双塔系统学习不好新物品和长尾物品
 ![[Pasted image 20240227174016.png]]