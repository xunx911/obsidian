# 举例的数字都是随便说的&论文

- 对比学习 Tiansheng Yao et al. Self-supervised Learning for Large-scale Item Recommendations. In CIKM, 2021
- MMoE(Google) Jiaqi Ma et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts. In KDD, 2018
- 解决极化问题(Youtube) Zhe Zhao et al. Recommending What Video to Watch Next: A Multitask Ranking System. In RecSys, 2019
- 时长建模(Youtube) Paul Covington, Jay Adams, & Emre Sargin. Deep Neural Networks for YouTube Recommendations. In RecSys, 2016
- 三塔模型(阿里)  Zhe Wang et al. COLD: Towards the Next Generation of Pre-Ranking System. In DLPKDD, 2020.
- 交叉网络 Ruoxi Wang et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.
- DIN(阿里) Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.
- SIM  Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020
- clip Radford et al. Learning transferable visual models from natural language supervision. In ICML, 2021

# 基础知识

## 转换流程

![[Pasted image 20240220114949.png]]
抖音没有曝光和点击,因为只能下滑看到一条视频

## 消费指标

![[Pasted image 20240220115204.png]]
不能把点击率作为唯一的优化指标,不然骗点击的标题会泛滥
追求短期消费指标是有意义的,但并不是衡量推荐系统好坏的根本指标(竭泽而渔,用户可能很快失去兴趣,而提高多样性虽然不会提升点击率,但可以提升用户粘性)

## 北极星指标

![[Pasted image 20240220115858.png]]
最关键的指标
用户规模(简单易懂,强相关)
消费与点击率冲突时,例如把多样性做好,用户点击率下降了但是使用了更长的时间,完全ok
发布(冷启动)

## 实验流程

![[Pasted image 20240220120802.png]]
小流量AB测试:对照组和实验组,判断新策略是否显著优于旧策略,如果是则加大流量,最终推全

# 推荐系统的链路

![[Pasted image 20240220121658.png]]
- 召回:用户刷新小红书时,系统调用几十条召回通道,每条通道召回几十到几百篇笔记,一共取回几千篇笔记
- 粗排:用规模小的机器学习模型排序和截断
- 精排:大规模神经网络模型打分,排序截断(小红书不截断)
- 重排:根据精排分数和多样性分数随机抽样得到几十篇笔记,然后把相似内容打散,并且插入广告和运营内容

## 召回通道

![[Pasted image 20240220122347.png]]
召回后还要过滤(例如不喜欢的笔记不喜欢的作者等)

## 重排

![[Pasted image 20240220122651.png]]
主要是考虑多样性,还要把相似的内容打散
![[Pasted image 20240220123018.png]]
抽样的依据:精排分数,多样性

## 粗排 精排

![[Pasted image 20240220122938.png]]

## 总结

![[Pasted image 20240220123114.png]]粗排时也会有一些规则,保证进入精排的笔记具有多样性
重排的规则非常复杂

# A/B测试

离线实验结果有收益,线上未必
![[Pasted image 20240220123422.png]]

## 随机分桶

![[Pasted image 20240220123609.png]]![[Pasted image 20240220123618.png]]每个桶不同的策略/参数

## 分层实验

目标:解决流量不够用的问题
![[Pasted image 20240220123844.png]]
![[Pasted image 20240220123905.png]]
一般来说不同层的实验不会对另一层造成影响,所以是正交
举例
![[Pasted image 20240220124024.png]]![[Pasted image 20240220124216.png]]不同的召回实验不能同时作用于一个用户身上
![[Pasted image 20240220124255.png]]![[Pasted image 20240220142607.png]]

## Holdout机制

![[Pasted image 20240220142950.png]]

## 实验推全 & 反转实验

![[Pasted image 20240220151612.png]]![[Pasted image 20240220151634.png]]
![[Pasted image 20240220151622.png]]![[Pasted image 20240220151710.png]]

## 总结

![[Pasted image 20240220151753.png]]

# 召回

## ItemCF

![[Pasted image 20240220163747.png]]
![[Pasted image 20240220152047.png]]从用户的行为中挖掘物品的相似性
![[Pasted image 20240220152158.png]]![[Pasted image 20240220152204.png]]![[Pasted image 20240220152256.png]]

### 完整流程

![[Pasted image 20240220160903.png]]![[Pasted image 20240220161040.png]]
⽤索引，离线计算量⼤，线上计算量⼩

## Swing召回通道(ItemCF变体)

### ItemCF的不足

![[Pasted image 20240220163349.png]]假设两篇不相关的笔记恰巧分享到了一个vx群,导致一些用户同时交互过这两篇笔记,系统就会误判

### Swing模型

![[Pasted image 20240220163909.png]]![[Pasted image 20240220163917.png]]

### 总结

![[Pasted image 20240220163933.png]]

## UserCF

![[Pasted image 20240220164133.png]]![[Pasted image 20240221145714.png]]![[Pasted image 20240221145722.png]]
越热门的物品应该影响越小
![[Pasted image 20240221150012.png]]![[Pasted image 20240221150021.png]]

### 完整流程

![[Pasted image 20240221150151.png]]
![[Pasted image 20240221150234.png]]

## 矩阵补充

![[Pasted image 20240221160442.png]]![[Pasted image 20240221160458.png]]![[Pasted image 20240221160519.png]]![[Pasted image 20240221160536.png]]![[Pasted image 20240221160615.png]]![[Pasted image 20240221160712.png]]![[Pasted image 20240221160725.png]]![[Pasted image 20240221160824.png]]![[Pasted image 20240221160922.png]]
![[Pasted image 20240221161105.png]]线上实时计算不现实->如何加速最近邻查找避免暴力枚举

### 近似最近邻查找

几亿个物品只做几万次内积
![[Pasted image 20240221161250.png]]![[Pasted image 20240221163202.png]]给物品向量划分区域,每个区域用一个单位向量表示,建立索引,作为key,能取回区域中的所有点
![[Pasted image 20240221163424.png]]
先找最近的区域,再计算区域内的点即可

### 总结

![[Pasted image 20240221163510.png]]![[Pasted image 20240221163524.png]]

## 双塔模型

![[Pasted image 20240221163651.png]]用户离散特征:例如喜欢的话题,所在的城市,性别等,每个特征单独用一个embedding层
用户连续特征:例如金额等
![[Pasted image 20240221163821.png]]
与矩阵补充相比,双塔模型使用了更多特征
![[Pasted image 20240221163928.png]]

### 训练

![[Pasted image 20240221164031.png]]

#### 正负样本的选择

![[Pasted image 20240221164112.png]]
负样本的选择比较讲究,看论文

#### pointwise

![[Pasted image 20240221164216.png]]

#### pairwise

![[Pasted image 20240221164349.png]]
![[Pasted image 20240221164413.png]]![[Pasted image 20240221164449.png]]
m是超参数,需要调
![[Pasted image 20240221171044.png]]

#### listwise

![[Pasted image 20240221171740.png]]![[Pasted image 20240223103247.png]]

### 总结

![[Pasted image 20240223103449.png]]
![[Pasted image 20240223112538.png]]
前期融合,通常是排序,双塔是后期融合

### 线上召回和更新

![[Pasted image 20240223123806.png]]![[Pasted image 20240223123816.png]]注意用户的特征向量是线上算的,而不用离线存储
![[Pasted image 20240223123917.png]]![[Pasted image 20240223123924.png]]![[Pasted image 20240223124120.png]]

#### 更新

![[Pasted image 20240223124140.png]]
增量更新可以几小时的更新,例如早上刷小红书,有了新的兴趣
![[Pasted image 20240223124358.png]]增量更新只更新用户塔的embedding层,全量更新才会更新全连接层.
![[Pasted image 20240223125027.png]]![[Pasted image 20240223125034.png]]全量更新用的是全量模型而不是增量模型

>[!question] 能否只做增量更新，不做全量更新?

no,效果不好,短时间的数据有偏差,与全天数据相比偏差巨大
![[Pasted image 20240223125603.png]]

### 正样本

选对正负样本,作用大于改进模型结构

![[Pasted image 20240223112914.png]]抛弃正样本的概率与样本的点击次数正相关

### 负样本

![[Pasted image 20240223122123.png]]

#### 简单负样本

![[Pasted image 20240223122159.png]]![[Pasted image 20240223122227.png]]![[Pasted image 20240223122417.png]]![[Pasted image 20240223122428.png]]
对于第一个用户来说,第二个物品相当于全体中随机抽样的,大概率不喜欢![[Pasted image 20240223122606.png]]打压负样本过大了
![[Pasted image 20240223122720.png]]训练时做调整来纠偏,线上召回时依然只用cos

#### 困难负样本

![[Pasted image 20240223122845.png]]困难指的是分类难
![[Pasted image 20240223123044.png]]

#### 常见错误

![[Pasted image 20240223123234.png]]
>[!caution] 曝光但没有点击的样本不能作为训练召回模型的负样本

![[Pasted image 20240223123318.png]]
召回是为了区分不感兴趣和比较感兴趣的
而排序是为了区分比较感兴趣和非常感兴趣的

#### 总结

![[Pasted image 20240223123545.png]]

### 总结

![[Pasted image 20240223125823.png]]![[Pasted image 20240223125936.png]]![[Pasted image 20240223125943.png]]

### 自监督学习

目的:优化物品塔
![[Pasted image 20240223131044.png]]![[Pasted image 20240223135342.png]]![[Pasted image 20240223135346.png]]![[Pasted image 20240223135448.png]]![[Pasted image 20240223135605.png]]![[Pasted image 20240223135919.png]]
自监督学习将物品转换为不同的表征,相同物品应该近,不同的应该远![[Pasted image 20240223140150.png]]![[Pasted image 20240223140159.png]]![[Pasted image 20240223140206.png]]![[Pasted image 20240223140304.png]]
原本可能是数码embedding和摄影embedding做计算,mask后直接丢掉变成缺失值的default
![[Pasted image 20240223140425.png]]
区别:mask是丢掉所有
![[Pasted image 20240223140457.png]]![[Pasted image 20240223141927.png]]
特征之间存在关联,所以要一起mask掉
![[Pasted image 20240223142040.png]]![[Pasted image 20240223142125.png]]![[Pasted image 20240223142159.png]]![[Pasted image 20240223142210.png]]![[Pasted image 20240223142355.png]]![[Pasted image 20240223142402.png]]![[Pasted image 20240223142743.png]]

#### 总结

![[Pasted image 20240223142806.png]]![[Pasted image 20240223142932.png]]
>[!note] 自监督效果非常好

## Deep Retrieval

### 区别

![[Pasted image 20240223143118.png]]
当作路径,不做相似最近邻查找
![[Pasted image 20240223143311.png]]

### 索引

![[Pasted image 20240223143431.png]]![[Pasted image 20240223143524.png]]
给定一个路径 取回多个物品作为召回的结果

### 预估模型

![[Pasted image 20240223143644.png]]![[Pasted image 20240223145423.png]]![[Pasted image 20240223145435.png]]
p1,p2,p3对应 L1,L2,L3

### 线上召回

![[Pasted image 20240223150110.png]]

### Beam Search

![[Pasted image 20240223150814.png]]![[Pasted image 20240223151217.png]]![[Pasted image 20240223151230.png]]![[Pasted image 20240223151237.png]]![[Pasted image 20240223152432.png]]![[Pasted image 20240223153606.png]]![[Pasted image 20240223153629.png]]![[Pasted image 20240223153638.png]]![[Pasted image 20240223153705.png]]

### 训练

![[Pasted image 20240223163749.png]]![[Pasted image 20240223163800.png]]![[Pasted image 20240223164025.png]]![[Pasted image 20240223164158.png]]![[Pasted image 20240223164207.png]]![[Pasted image 20240223170143.png]]![[Pasted image 20240223170552.png]]
固定J-1条路径,物品与pathl的相关性越高,loss越小

#### 小结

交替训练更新
![[Pasted image 20240223171630.png]]更新神经网络时物品作为中介
![[Pasted image 20240224125915.png]]用户作为中介,给定一个物品,找到点击过物品的所有用户,然后神经网络计算用户对路径的兴趣分数,把分数加起来就是物品与路径的相关性

#### 总结

召回分两阶段,用户->路径,路径->物品
![[Pasted image 20240224130156.png]]
![[Pasted image 20240224130232.png]]
>[!note]  双塔模型本质是用向量表征作为用户和物品之间的中介,而DR是path

![[Pasted image 20240224143917.png]]

## 其他召回路径

### 地理位置召回

![[Pasted image 20240224175012.png]]
经纬度编码成二进制哈希码
给定用户的geohash,召回附近区域的新的优质笔记,得是优质笔记
![[Pasted image 20240224175139.png]]
哪些笔记用户感兴趣留待排序模型判断
![[Pasted image 20240224175219.png]]

### 作者召回

![[Pasted image 20240224175410.png]]![[Pasted image 20240224175415.png]]![[Pasted image 20240224175549.png]]
相似作者的相似度计算,例如粉丝的重合度

### 缓存召回

![[Pasted image 20240224175629.png]]
![[Pasted image 20240224175753.png]]
比较复杂的规则,例如扶持低曝光的笔记,设计规则让低曝光的多保留

## 曝光过滤&Bloom Filter

![[Pasted image 20240224180306.png]]
![[Pasted image 20240224180342.png]]
![[Pasted image 20240226145540.png]]
是二进制向量,要么0要么1
>[!question] 如何用k个哈希函数做曝光过滤

![[Pasted image 20240226145814.png]]
初始是m维的向量,把已曝光的物品id哈希到的位置置为1
![[Pasted image 20240226150231.png]]
召回的物品映射的三种情况
![[Pasted image 20240226150331.png]]
k=3时
只要有一个位置不为1 就未曝光
![[Pasted image 20240226150447.png]]

### 误伤概率

![[Pasted image 20240226150539.png]]![[Pasted image 20240226150921.png]]

### 曝光过滤的链路

![[Pasted image 20240226151216.png]]
实时流处理的要求很高

### 缺点

![[Pasted image 20240226151247.png]]想要删除一个物品,需要重新计算整个集合的二进制向量

# 排序

## 多目标排序模型

先不区分粗排和精排
![[Pasted image 20240226153430.png]]
![[Pasted image 20240226153458.png]]
![[Pasted image 20240226153542.png]]权重是做A/B测试调出来的
加权和也有改进
![[Pasted image 20240226153736.png]]
用户特征:用户id  用户画像
物品特征:物品id  物品画像  作者信息
统计特征:用户统计特征(比如用户在过去30天中曝光了多少篇笔记  点击了多少篇笔记)  物品统计特征(被曝光了多少次,点击点赞了多少次)
场景特征:当前时间(周末,工作日)  用户所在地点(同城)

### 训练

![[Pasted image 20240226154102.png]]模型做出的预估拟合真实目标
y是0或1,例如1 0 0 1代表有点击 无点赞 无点赞 有转发
二元分类 用交叉熵
![[Pasted image 20240226154247.png]]

### 预估值校准

>[!question] 为什么要做校准

![[Pasted image 20240226154441.png]]
![[Pasted image 20240226161618.png]]

### Multi-gate Mixture-of-Experts(MMoE)

![[Pasted image 20240226162854.png]]
三个神经网络不共享参数,实践中通常4个或者8个
![[Pasted image 20240226162955.png]]特征向量输入另一个神经网络,输出对应三个专家的权重
![[Pasted image 20240226163046.png]]
q123也是后续加权的权重
![[Pasted image 20240226163135.png]]
这些神经网络都是1个或多个全连接层(示例)
假如有10个目标,就有10个权重
专家数量是超参数,需要调

#### 极化现象(Polarization)

实践中会出现下列现象
![[Pasted image 20240226163356.png]]
![[Pasted image 20240226163443.png]]
![[Pasted image 20240226163507.png]]强迫部分专家做预测,因为如果发生极化现象,那么那个专家被mask时结果会非常差,所以可以避免极化
MMoE不是用了就有效果的,可能是实现不够好或者业务不够匹配,效果不好的原因不清楚

### 融合预估分数

![[Pasted image 20240226164820.png]]
简单又常用
![[Pasted image 20240226170322.png]]
$p_{time}$的意思是预估用户观看时长,$\alpha_1,\alpha_2$都是超参数,A/B测试调
有很多预估指标,连乘
![[Pasted image 20240226170508.png]]
以$p_{time}$为例,时长最长,排名越小,得分越高,与之前不同,这里用的是排名
![[Pasted image 20240226170731.png]]
$\alpha$是超参数,如果都是1,则是营收,有很明确的物理意义

### 视频播放建模

#### 视频播放时长

![[Pasted image 20240226171012.png]]
![[Pasted image 20240226171104.png]]

#### Youtu的时长建模

![[Pasted image 20240226171207.png]]
这个神经网络被所有任务共享
以播放时长为例
t的意思是用户实际观看视频的时长
t越大,y越大
用p拟合y,最小化交叉熵,如果p=y,则exp(z)=t所以可以用exp(z)作为播放时长的预估
![[Pasted image 20240226183603.png]]
 所以训练时用p 训练完成后就没用了, 线上计算时用exp(z)即可
 ![[Pasted image 20240226184437.png]]
 ![[Pasted image 20240226184506.png]]
 #### 视频完播

![[Pasted image 20240226184659.png]]![[Pasted image 20240226184723.png]]
![[Pasted image 20240226184821.png]]
实践中不能将完播率直接用在融分公式中,否则对短视频有利而对长视频不利
![[Pasted image 20240226185014.png]]

## 排序模型的特征

### 用户画像

![[Pasted image 20240226185145.png]]
用户id的embedding通常32维  64维
人口统计学信息  账号信息相差都很大
感兴趣的类目可能是用户提取的  也有可能是算法提取的
用户id和物品id虽然本身不携带额外信息,但是embedding的价值都非常高

### 物品画像

![[Pasted image 20240226185712.png]]
GeoHash是经纬度的编码,表示一个长方形的区域
内容信息量  图片美学是算法打分  事先训练nlp和cv模型

### 用户统计特征

![[Pasted image 20240226185834.png]]

### 笔记统计特征

![[Pasted image 20240226185915.png]]
例如笔记30天指标很高,但是最近1天的指标很差,说明过时了,不应该给更多流量

### 场景特征

![[Pasted image 20240226190111.png]]
随用户请求传来的
安卓和苹果用户的点赞差异也非常显著

### 特征处理

![[Pasted image 20240226190237.png]]
离散特征处理起来比较容易  存储占用较小
连续特征平滑解决异常值的问题,例如一些曝光特别多的笔记,数百万 直接计算会导致梯度计算等很奇怪
平滑点赞率去掉偶然性造成的波动

### 小结

![[Pasted image 20240226190549.png]]
各个特征的各个特征都很有用

### 特征覆盖率

![[Pasted image 20240226190616.png]]

### 数据服务

3个数据源,存储在内存服务器中
![[Pasted image 20240226190720.png]]![[Pasted image 20240226190800.png]]
![[Pasted image 20240226190816.png]]用户画像存储库压力小  物品画像数据库压力非常大
所以用户存储库可以特征多 大一些  而物品最好不要放很大的向量
这两个静态有时可以缓存在排序服务器本地加速计算
但是统计数据不行,要尽快刷新数据库
![[Pasted image 20240226191607.png]]
tensorflow返回分数,排序服务器用融合分数 多样性分数 业务规则给笔记排序,最后把分最高的返回
实际上的系统会复杂很多

## 粗排

### 粗排vs精排

![[Pasted image 20240226191743.png]]

### 回顾精排&双塔模型

![[Pasted image 20240226191846.png]]
shared bottom
![[Pasted image 20240226191908.png]]
![[Pasted image 20240226191934.png]]
![[Pasted image 20240226191958.png]]

### 粗排的三塔模型

![[Pasted image 20240226192146.png]]交叉特征是指用户特征和场景特征交叉
介于前期融合和后期融合之间(前期融合是把最下面的特征concatenation,而这里是把输出的三个特征concatenation)
![[Pasted image 20240226192400.png]]物品特征相对稳定,可以缓存物品塔向量,每隔一段时间刷新一次
每当一个一个用户点击  一个物品曝光,统计特征就发生了变化,所以交叉塔必须小,通常来说交叉塔只有一层,宽度也比较小
![[Pasted image 20240226192730.png]]
代价>交叉塔的n次推理
![[Pasted image 20240226192804.png]]
粗排模型的理念是尽量减少线上的计算量

# 特征交叉

## Factorized Machine(FM)

几年前比较流行,现在已经不常用了

### 线性模型

![[Pasted image 20240226193057.png]]
特征之间没有交叉

### 二阶交叉特征

![[Pasted image 20240226193233.png]]
例如房子的面积和房价,就需要交叉
如果d很大 计算代价就太大了,而且容易过拟合
![[Pasted image 20240226193421.png]]
U是对称矩阵,用V来近似
![[Pasted image 20240226193515.png]]
如此 就减少了参数量
![[Pasted image 20240226194519.png]]
![[Pasted image 20240226194539.png]]在推荐系统中,FM显著比线性模型效果好,2010时逻辑回归加FM效果很好,现在过时了

## 深度交叉网络(DCN)

用来代替简单的全连接网络

### 之前的模型替换

![[Pasted image 20240226195924.png]]![[Pasted image 20240226195945.png]]![[Pasted image 20240226195957.png]]

### 交叉层(Cross Layer)

![[Pasted image 20240226200107.png]]
第i个交叉层
把xi输入全连接层得到y形状相同,与x0做哈达玛乘积(逐元素相乘),等到z与x0形状一样,与xi相加,类似与resnet中的跳跃连接skip connection
![[Pasted image 20240226200342.png]]
xi+1是输出,x0和xi是输入
参数全在全连接层
![[Pasted image 20240226200432.png]]

### 交叉网络(Cross Network)

![[Pasted image 20240226200627.png]]![[Pasted image 20240226200642.png]]
x0需要输入到后面的交叉层
![[Pasted image 20240226200745.png]]

### 深度交叉网络(Deep & Cross Network)

![[Pasted image 20240226200907.png]]
concatenation后输入到两个网络中,各输出一个向量,concatenation后输入到全连接层,输出一个向量
![[Pasted image 20240226201028.png]]
比全连接网络效果更好,召回和排序中都可以使用

## LHUC网络结构

有效但只能用于精排
![[Pasted image 20240227153940.png]]![[Pasted image 20240227153954.png]]

### 语音识别中的LHUC

![[Pasted image 20240227154111.png]]神经网络包含多个全连接层,最后得到的每个向量都介于0到2
![[Pasted image 20240227154153.png]]
哈达玛乘积之后输入全连接层
神经网络跟上面一样,
![[Pasted image 20240227154248.png]]
语音信号中的特征结合说话者的特征,做到了个性化
示例中是2层   实践中可以更深
![[Pasted image 20240227154331.png]]
这样的介于0到2的向量与信号特征做乘积就可以放大一些特征,缩小一些特征,做到个性化

### 推荐系统排序模型中的LHUC

![[Pasted image 20240227154433.png]]

## SENet & Bilinear Cross

对排序有收益
![[Pasted image 20240227154610.png]]![[Pasted image 20240227154624.png]]
![[Pasted image 20240227154703.png]]
设m维特征,每个特征是一个k维向量,所有特征是m\*k的矩阵
做完avgpool之后的向量,每个元素对应一个离散特征,比如第一个元素对应用户id
r为压缩比例>1
然后再恢复
![[Pasted image 20240227154851.png]]
用m\*1维的向量做加权
![[Pasted image 20240227154935.png]]
例如学出物品id embedding对任务重要性不高,就给它降权
![[Pasted image 20240227155052.png]]维度可以不同,毕竟只是加权
![[Pasted image 20240227155149.png]]
也就是说,SENet会根据所有特征,判断特征的重要性

### Field间特征交叉

![[Pasted image 20240227155310.png]]xi和xj是两个特征,   哈达玛乘积的代价有些大,不过都要求形状一样
![[Pasted image 20240227155800.png]]![[Pasted image 20240227155948.png]]形状可以不同,参数矩阵太大,需要人工选pair

![[Pasted image 20240227160027.png]]
人工指定特征交叉

![[Pasted image 20240227160121.png]]

### FIBiNet

![[Pasted image 20240227160200.png]]![[Pasted image 20240227160713.png]]
作者认为最下层的Bilinear cross是多余
![[Pasted image 20240227160746.png]]
与连续特征concatenate输入

# 用户行为序列

## 用户行为序列建模

last n意思用户最后交互过的n个物品
很有效,用于召回和排序模型  指标都会大涨
![[Pasted image 20240227160949.png]]![[Pasted image 20240227160957.png]]
最后平均作为用户的一种特征

### LastN特征

![[Pasted image 20240227161022.png]]

#### 小红书的实践

 ![[Pasted image 20240227161121.png]]
 attention的效果比取平均效果好,但是计算量大
 最后把向量拼起来
 不止物品id  也有物品类目等等
 每家公司用的方法都不太一样,主要原因是系统基建水平不一样
 ## DIN模型
 ![[Pasted image 20240227161343.png]]
 ![[Pasted image 20240227161444.png]]
 红色是用户交互过的lastN物品的向量表征,
 蓝色是  比如粗排选出物品就是精排的候选物品
 ![[Pasted image 20240227161602.png]]
 分别计算相似度
 ![[Pasted image 20240227161616.png]]
 把$\alpha与x$相乘,得到lastN向量的加权和,权重就是相似度
 ![[Pasted image 20240227161723.png]]![[Pasted image 20240227162329.png]]![[Pasted image 20240227162350.png]]![[Pasted image 20240227162523.png]]
 ## SIM模型
 主要目的是保留用户的长期兴趣
 ### DIN的缺点
 ![[Pasted image 20240227162642.png]]
 做过实验,提升用户行为序列的长度,能提升所有指标,但是不划算,增加的计算量太大,收益不够高
 ![[Pasted image 20240227162818.png]]
 因为无关的物品权重接近0,所以排除掉也可以,这样就可以降低计算量
 ![[Pasted image 20240227162924.png]]
 如果不用SIM,n可能就是一两百
 topk  例如用类目信息把1000个变成了100个
 ### 查找
 ![[Pasted image 20240227163102.png]]
 除非基建很强,hard search就可以了
 ### 注意力机制
 ![[Pasted image 20240227163214.png]]
 紫色向量几乎不变,被排除的物品相似度本就接近0
 ![[Pasted image 20240227163251.png]]
 小trick   把两个向量concatenation
 ![[Pasted image 20240227163345.png]]
 候选物品不需要时间的embedding
 ![[Pasted image 20240227163455.png]]
 使用时间信息能得到显著提升
 ![[Pasted image 20240227163520.png]]
 # 多样性
 ## 推荐系统中的多样性
 ### 物品相似度的度量
 ![[Pasted image 20240227173843.png]]
 ![[Pasted image 20240227173855.png]]
 ![[Pasted image 20240227173920.png]]
 头部现象严重,双塔系统学习不好新物品和长尾物品
 ![[Pasted image 20240227174016.png]]
 图片一个向量 文本一个向量,拼起来,但是没有数据集训练
 ![[Pasted image 20240301154019.png]]
 ![[Pasted image 20240301154123.png]]
 ### 提升多样性的方法

![[Pasted image 20240301154421.png]]
![[Pasted image 20240301154432.png]]粗排之后也需要多样性
小红书优化之后效果好

## Maximal Marginal Relevance(MMR)

from搜索排序
![[Pasted image 20240301154810.png]]![[Pasted image 20240301154824.png]]
红色是已经选中的物品
MMR: argmax MRi
![[Pasted image 20240301154956.png]]
![[Pasted image 20240301161027.png]]
没必要让第30个物品和第一个物品不相似,用户都忘记之前的了

## 重排的规则

重排的规则非常多,举几种典型的例子,数字都是随便编的
![[Pasted image 20240301161142.png]]
规则的优先级高于多样性算法
![[Pasted image 20240301161412.png]]
  ![[Pasted image 20240301161748.png]]
  ### MMR+重排规则
  ![[Pasted image 20240301161838.png]]
  ## DPP
  ### 数学基础
  目标是从集合中选出多样性的物品
  DPP是目前公认的多样性最好的算法
  #### 超平行体
  ![[Pasted image 20240301162140.png]]
  ![[Pasted image 20240301162202.png]]![[Pasted image 20240301162320.png]]
  vi是边
  ![[Pasted image 20240301172200.png]]![[Pasted image 20240301184830.png]]![[Pasted image 20240301184841.png]]![[Pasted image 20240301184856.png]]
  v是clip的向量
  ![[Pasted image 20240301185901.png]]
  ### 多样性算法

![[Pasted image 20240302114011.png]]![[Pasted image 20240302114031.png]]![[Pasted image 20240302114516.png]]![[Pasted image 20240302115532.png]]![[Pasted image 20240302115539.png]]
准确求解不可能, 使用贪心算法近似,每一轮选一个物品i

#### 求解DPP

![[Pasted image 20240302120012.png]]计算行列式需要cholesky或者特征分解需要$O(|S|^3)$
![[Pasted image 20240302120408.png]]
还需要加上计算A的时间
>[!caution] 系统留给多样性算法的时间10ms左右

![[Pasted image 20240302150553.png]]
 ![[Pasted image 20240302150654.png]]
 As对称半正定,给矩阵添加一行和一列,不用重算Cholesky分解,而是可以快速算出变化
 ![[Pasted image 20240302151203.png]]![[Pasted image 20240302151936.png]]
 #### DPP的扩展
![[Pasted image 20240302152139.png]]![[Pasted image 20240302152236.png]]![[Pasted image 20240302152247.png]]

# 冷启动

推荐系统中最复杂最困难的一环

## 评价指标

![[Pasted image 20240302152727.png]]
UGC-PGC    用户上传的-采购的内容上传的
![[Pasted image 20240302152806.png]]
如果用正常的推荐链路,新笔记很难得到曝光
![[Pasted image 20240302152855.png]]
![[Pasted image 20240302152916.png]]![[Pasted image 20240302153047.png]]
作者侧和用户侧指标是通用的,内容侧指标只有一些企业在用

### 作者侧指标

#### 发布渗透率

![[Pasted image 20240302153229.png]]

#### 人均发布量

![[Pasted image 20240302153250.png]]
![[Pasted image 20240302153302.png]]

### 用户侧指标

#### 新笔记的消费指标

![[Pasted image 20240302153414.png]]多关注低曝光笔记

#### 大盘消费指标

![[Pasted image 20240302153614.png]]
目标是不伤害大盘消费指标

### 内容侧指标

#### 高热笔记占比

![[Pasted image 20240302153729.png]]

### 优化点

![[Pasted image 20240302153754.png]]
流量向新笔记倾斜,但也不要伤害用户体验,因为新笔记推荐不准

## 简单的召回通道

### 召回的难点

#### 召回的依据

![[Pasted image 20240302153951.png]]
没有交互就走不了itemCF这种通道
![[Pasted image 20240302154346.png]]
>[!caution] 双塔模型是召回最重要的通道,没有之一,离开双塔模型很难做好推荐

没有idembedding对排序也有影响

#### 召回通道

![[Pasted image 20240302154620.png]]类目和关键词在笔记新发布时是最有用的,但是发布一段时间之后会失效

### 双塔模型

![[Pasted image 20240302154758.png]]

####  ID Embedding

![[Pasted image 20240302155757.png]]
这个默认向量是学出来的,比随机初始化更好
![[Pasted image 20240302155843.png]]
相似:图文内容,类目,关键词

#### 多个向量召回池

![[Pasted image 20240302155928.png]]

### 类目召回

![[Pasted image 20240302160001.png]]

#### 基于类目的召回

![[Pasted image 20240302160028.png]]
![[Pasted image 20240302160042.png]]

#### 基于关键词的召回

![[Pasted image 20240302160131.png]]

#### 缺点

![[Pasted image 20240302160146.png]]
留给每篇笔记的窗口期很短

### 聚类召回

基于笔记的图文内容,在冷启动时很有用
![[Pasted image 20240302160347.png]]

#### 索引

![[Pasted image 20240302160431.png]]![[Pasted image 20240302161101.png]]
缺点和类目召回一样,都是只对刚刚发布的新笔记有效,发布几小时之后就不太可能被召回了

#### 内容相似度模型

![[Pasted image 20240302161238.png]]
![[Pasted image 20240302161255.png]]
全连接层是需要学习的
cnn和bert是预训练好的,可以做finetune

##### 训练模型

![[Pasted image 20240302161409.png]]
与双塔模型类似 每个训练样本都是个三元组
这三个神经网络的参数是相同的
![[Pasted image 20240302161500.png]]

##### 正负样本的选取

![[Pasted image 20240302164003.png]]![[Pasted image 20240302164013.png]]![[Pasted image 20240302164833.png]]

### Look-Alike人群扩散

#### 起源于互联网广告

![[Pasted image 20240302165953.png]]![[Pasted image 20240302170008.png]]
满足所有条件的受众被称为种子用户,不会很多,可能几万人, 但是潜在的可能很多,比如要投100万人的广告
![[Pasted image 20240302170110.png]]
![[Pasted image 20240302170119.png]]

#### 新笔记召回

各主要指标都有显著提升
![[Pasted image 20240302170216.png]]![[Pasted image 20240302170235.png]]
少数对新笔记有交互的用户作为种子用户,用双塔模型学到的种子用户向量的均值作为新笔记的表征
![[Pasted image 20240302170500.png]]近线更新是指不用做实时的更新,分钟级的就可以了
![[Pasted image 20240302170616.png]]用用户的向量在存储新笔记的向量数据库中作为query进行查找,这个召回通道就是look-alike
![[Pasted image 20240302170757.png]]

## 流量调控

![[Pasted image 20240302170849.png]]尽量让新笔记得到曝光,也要尽可能准,不要让用户反感
![[Pasted image 20240302170939.png]]![[Pasted image 20240302171650.png]]
超过30天的笔记只能通过搜索和其他渠道曝光
自然分发:一视同仁,不扶持
![[Pasted image 20240302171820.png]]
3:通过更复杂的提权策略,保证每篇笔记比如24h内获得100次曝光
4:根据内容质量,决定不同的保量目标

### 新笔记提权boost

![[Pasted image 20240302172130.png]]
![[Pasted image 20240302172139.png]]
干涉粗排和重排(漏斗)
![[Pasted image 20240302172322.png]]
需要认为设置一些提权系数,很难精确控制

### 新笔记保量

![[Pasted image 20240302185331.png]]
真正做的时候数字需要仔细调才行

![[Pasted image 20240302185432.png]]![[Pasted image 20240302190654.png]]![[Pasted image 20240302190715.png]]![[Pasted image 20240302190824.png]]![[Pasted image 20240302190840.png]]
粗暴提权对新笔记并不好

### 差异化保量

![[Pasted image 20240302191053.png]]![[Pasted image 20240302191111.png]]![[Pasted image 20240302191126.png]]
保量虽然不是非常先进的技术,但是也只有一线大厂能做到,做好了对发布侧(作者侧)的指标有非常大的提升

## AB测试

冷启动的AB测试非常复杂
![[Pasted image 20240302200304.png]]
冷启动的AB测试需要除大盘指标以外很多指标,很麻烦
![[Pasted image 20240302200935.png]]

### 用户侧实验

![[Pasted image 20240302200957.png]]

#### 缺点

![[Pasted image 20240302201036.png]]
>[!question] 为什么会出现推全之后diff缩小的现象?

![[Pasted image 20240302201859.png]]
因为实验组更多的曝光了新笔记,根据保量,新笔记在对照组中曝光会减少,所以一个变好一个变差,所以diff大,推全后没那么大

### 作者侧实验

不太好做,没有完美的方案

#### 方案一

![[Pasted image 20240302202101.png]]
比如观察作者的发布意愿
![[Pasted image 20240302202148.png]]
例子简单保量-差异化保量,最后对比两组作者的发布指标
![[Pasted image 20240302202227.png]]
因为新笔记曝光还是3分之1,实验组会抢对照组的曝光,只是新笔记里出现了diff,比如例子里权重×2,显然推全后没变化

如果是新老笔记自由竞争
![[Pasted image 20240302203243.png]]![[Pasted image 20240302203252.png]]
测试时一份新笔记抢两份老笔记的流量,推全后一份新笔记抢一份老笔记的流量, 所以设定发生了变化, 测试结果会有差异

#### 方案二

![[Pasted image 20240302203546.png]]避免两组新笔记抢流量,比方案一更可信
![[Pasted image 20240302203823.png]]
新笔记池减少一半,肯定会影响用户体验,为了AB测试影响了大盘指标

#### 方案三

![[Pasted image 20240302204049.png]]精准上这种方案最好,但是不可行,消费指标一定大跌

### 总结

既需要用户侧实验,又需要作者侧实验,因为既需要用户满意,也需要激励作者发布
![[Pasted image 20240302204219.png]]

# 涨指标的方法

## 推荐系统的评价指标

这里讲信息流推荐(电商不一样,是营收)
![[Pasted image 20240302204522.png]]包括今天在内,LT取平均作为所有用户的指标
计算LT其实就是圈定今天活跃的用户,把他们的数量作为分母,他们总活跃天数作为分子,分子除以分母就是LT,把低活用户赶走,分子- 分母--,所以LT增长,实践中不仅要LT增长,还要DAU不下降
![[Pasted image 20240302205012.png]]
比如抖音长视频多,短视频少,用户时长增加了,但是看了更少的视频,曝光也减少了.
时长关乎DAU和留存,曝光数关乎广告收入,要平衡
![[Pasted image 20240302205300.png]]![[Pasted image 20240302205312.png]]

## 召回

![[Pasted image 20240303140626.png]]

### 双塔模型

#### 方向一:优化正负样本

![[Pasted image 20240303140932.png]]

#### 方向二:改进神经网络结构

![[Pasted image 20240303141019.png]]

##### 多向量模型

![[Pasted image 20240303141147.png]]
物品塔一样,
用户塔输出多个向量,与单向量模型不同
![[Pasted image 20240303141237.png]]有点类似于排序中的多目标模型,而单向量模型只是最后做相似度计算
>[!question] 为什么用户塔多个而物品塔一个

物品塔需要提前算出来,一个向量数据库代价小

#### 方向三:改进训练方法

![[Pasted image 20240303141519.png]]

### Item-to-Item(I2I)

推荐系统中I2I的重要性与双塔模型差不多,但是已经很成熟了,做不了太多改进
![[Pasted image 20240303141801.png]]![[Pasted image 20240303141822.png]]
四种模型召回的结果存在足够大的差异,所以结合起来效果更好
各自配额需要仔细调
![[Pasted image 20240303141933.png]]

### 小众的召回模型

#### 类似I2I的模型

![[Pasted image 20240303142016.png]]

#### 更复杂的模型

![[Pasted image 20240303142105.png]]
配额小但是确实有效果

### 总结

![[Pasted image 20240303142134.png]]
召回总量大到一定程度时,再投入会收益很小

## 排序模型

![[Pasted image 20240303142333.png]]

### 精排模型的改进

![[Pasted image 20240303142454.png]]
基座把原始特征映射到特征向量
全连接网络都不会深
![[Pasted image 20240303142549.png]]

#### 基座

![[Pasted image 20240303142619.png]]
百分之99的参数量都在embedding层
数据大而全连接层不够大,投入产出比的问题,工业界使用1-6层
 #### 多目标预估
 ![[Pasted image 20240303142827.png]]![[Pasted image 20240303142839.png]]![[Pasted image 20240303142907.png]]
 ### 粗排模型的改进
 ![[Pasted image 20240303143107.png]]
 #### 粗精排一致性建模
 ![[Pasted image 20240303143419.png]]![[Pasted image 20240303143428.png]]![[Pasted image 20240303143543.png]]
 ### 用户行为序列建模
 当排序模型优化到一定程度之后,涨指标会越来越难.这时候涨指标最主要的途径就是用户行为序列建模
 ![[Pasted image 20240303143730.png]]
 last-n取平均可以作为用户的一种特征
 ![[Pasted image 20240303143751.png]]
 ![[Pasted image 20240303143838.png]]
 目前长序列建模做的最好的是快手,他们可以使用100多万个物品,几乎能覆盖用户所有交互历史
 想做到超长序列其实非常难
 ![[Pasted image 20240303144422.png]]
 ### 在线学习
 ![[Pasted image 20240303144546.png]]![[Pasted image 20240303144553.png]]
 #### 资源消耗
 ![[Pasted image 20240303144745.png]]
 ![[Pasted image 20240303144840.png]]
 ![[Pasted image 20240303144921.png]]
 最好在模型成熟后再上在线学习
 ### 老汤模型
 解决不了的话,新模型很难超过老模型,开发迭代效率很低

![[Pasted image 20240303145039.png]]
#### 问题 1：如何快速判断新模型结构是否优于⽼模型?
![[Pasted image 20240303145305.png]]
只是判断谁的结构更好,10天训练的新模型,不可能好于训练100天的老模型
#### 问题2：如何更快追平线上的⽼模型？
![[Pasted image 20240303145426.png]]
### 总结
![[Pasted image 20240303145538.png]]
## 提升多样性
### 排序
![[Pasted image 20240303150618.png]]![[Pasted image 20240303150628.png]]![[Pasted image 20240303150855.png]]![[Pasted image 20240303151033.png]]
### 召回
#### 双塔模型：添加噪声
![[Pasted image 20240303151239.png]]
很神奇,添加噪声反而会提升指标
#### 双塔模型：抽样用户行为序列
![[Pasted image 20240303151333.png]]
这样相同的两次召回,结果也会不一样,随机性提高了多样性
>[!note] 现在n可以很大,但是计算代价不变
#### U2I2I：抽样用户行为序列
![[Pasted image 20240303151439.png]]
![[Pasted image 20240303151524.png]]
### 探索流量

![[Pasted image 20240303151624.png]]
因为不匹配所以只能boost或者强插
长期上可以挖掘用户的兴趣点,增加用户留存
### 总结
![[Pasted image 20240303151752.png]]
## 特殊对待特殊人群
![[Pasted image 20240303151927.png]]
对新用户和低活用户只需要考虑留存,是否消费是否看广告根本不重要
![[Pasted image 20240303152105.png]]
### 构造特殊的内容池
![[Pasted image 20240303152138.png]]![[Pasted image 20240303152618.png]]![[Pasted image 20240303152629.png]]![[Pasted image 20240303152639.png]]![[Pasted image 20240303152705.png]]所以增加内容池不会增加额外的训练代价,但是需要额外的推理代价
![[Pasted image 20240303152821.png]]
离线更新ann索引还是线上召回做ann检索都需要额外的算力
### 特殊的排序策略
#### 排除低质量物品
![[Pasted image 20240303152917.png]]
#### 差异化的融分公式
![[Pasted image 20240303153028.png]]
注意只能用在特殊人群身上,如果用在全体用户身上,损害用户指标
### 特殊的排序模型
#### 差异化的排序模型
![[Pasted image 20240303153252.png]]
![[Pasted image 20240303153424.png]]![[Pasted image 20240303153431.png]]
和MMoE的区别就是这里只用用户特征![[Pasted image 20240303153549.png]]
### 错误做法
![[Pasted image 20240303153655.png]]![[Pasted image 20240303153707.png]]
### 总结
![[Pasted image 20240303153824.png]]
## 利用交互行为
![[Pasted image 20240303153920.png]]
### 关注
![[Pasted image 20240303154046.png]]![[Pasted image 20240303154057.png]]![[Pasted image 20240303154115.png]]