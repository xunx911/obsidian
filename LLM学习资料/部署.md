目前推荐vllm
# vllm
[vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs (github.com)](https://github.com/vllm-project/vllm)
pip install之后就可以使用了
部署一行命令就搞定了,例如
```shell
vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --tp 2
```
主要就是指定模型路径，模型精度，几卡并行，其他可选参数见下
[Engine Arguments — vLLM](https://docs.vllm.ai/en/latest/models/engine_args.html
# ollama
[ollama/ollama: Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models. (github.com)](https://github.com/ollama/ollama)
ollama是以[ggerganov/llama.cpp: LLM inference in C/C++ (github.com)](https://github.com/ggerganov/llama.cpp)为基础的项目，也可直接使用llamacpp
自定义模型需要gguf格式，转换脚本是llamacpp里的`convert_xx_to_gguf.py`，优先去huggingface搜索其他人上传的，modelfile参考原模型的chat template进行修改

# lmdeploy
[InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs. (github.com)](https://github.com/InternLM/lmdeploy)
用起来也挺方便快速的，没有过多体验

# 对比
vllm会占满所有卡，常驻显存，ollama是一段时间没有请求进来就会退显存
ollama在我部署了两个实例之后，时不时会出现卡死的情况
[ollama vs vllm - 开启并发之后的 ollama 和 vllm 相比怎么样？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Bw4m1D7K3/?spm_id_from=333.337.search-card.all.click&vd_source=0c538d32ed02cb065b3f211a5235afe9)
省流：vllm在高并发下性能比ollama好