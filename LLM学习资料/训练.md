[从零详细地梳理一个完整的 LLM 训练流程-CSDN博客](https://blog.csdn.net/qq_27590277/article/details/131447830)
[LLM 预训练|SFT监督微调|推理_llm sft训练-CSDN博客](https://blog.csdn.net/Maxcu/article/details/137214383)
# 简介
简而言之，预训练是大量语料（TB级）的无监督训练，给一段文本，根据next token predict从头开始计算loss,train就完了。
sft（有监督微调）一般是少量语料（几千条（input，output））进行指令对齐，是给定input，从output开始next token predict，input的loss是不计算的，以适应对话格式或其他下游任务格式。
例如
```
给LLM一个选择题，预训练后的LLM可能并不会回答，而是重复选项，因为语料里的问答格式较少。而sft后的概率分布就调整到问答格式的了，可以进行回答。
```
注意训练阶段的next token predict是根据ground truth的，而不是自己一个一个生成的序列去和原序列计算loss，所以可以并行计算。
# 使用LLaMA-Factory
[hiyouga/LLaMA-Factory: A WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024) (github.com)](https://github.com/hiyouga/LLaMA-Factory)
这个项目基本都封装好了，需要准备的除了模型，只有数据和配置文件。
数据放在`data/`或其他路径，再将数据格式信息添加到`data/dataset_info`即可
[LLaMA-Factory/examples/README_zh.md at main · hiyouga/LLaMA-Factory (github.com)](https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md)
参考examples中的例子，进行配置，然后启动训练即可

# lora
>[arxiv.org/pdf/2106.09685](https://arxiv.org/pdf/2106.09685) 

[大模型高效微调-LoRA原理详解和训练过程深入分析_大模型 lora-CSDN博客](https://blog.csdn.net/m0_63171455/article/details/139614304)
简而言之，按我的理解，训练就是去更新权重矩阵，也就是有个$\Delta W$,对它进行低秩分解，分解成一个降维矩阵A，一个升维矩阵B，秩为r，参数量就从$m*n$降到了$m*r+n*r,r<<m,n$, 需要训练的参数量大大降低了，一般只占总参数量的万分之几到千分之几，很少

# chat template
[如何设置transformers的聊天模板chat_template？_pipeline 聊天模板-CSDN博客](https://blog.csdn.net/mingzai624/article/details/135952802)

# 模型文件
可以看看模型文件里的各个json文件 py文件，有助于理解，例如
[Qwen/Qwen2-7B-Instruct at main (huggingface.co)](https://huggingface.co/Qwen/Qwen2-7B-Instruct/tree/main)
